# Issue #8: Derin Ã–ÄŸrenme Modelinin TasarlanmasÄ±

**Durum:** TamamlandÄ±  
**Sorumlu:** Furkan PehlivanoÄŸlu  
**Tarih:** 29.04.2025

## ğŸ” AmaÃ§
Bu aÅŸamada projemiz iÃ§in uygun bir derin Ã¶ÄŸrenme modeli seÃ§ilmiÅŸ ve katman yapÄ±sÄ± detaylandÄ±rÄ±lmÄ±ÅŸtÄ±r. Projede kullanÄ±lan veri tipi ve problemin yapÄ±sÄ± gÃ¶z Ã¶nÃ¼ne alÄ±narak **MLP (Multi-Layer Perceptron)** modeli tercih edilmiÅŸtir. Modelin sÄ±nÄ±flandÄ±rma performansÄ±nÄ± artÄ±rmak adÄ±na farklÄ± aktivasyon fonksiyonlarÄ± ve katman kombinasyonlarÄ± deÄŸerlendirilmiÅŸtir.

## ğŸ§  SeÃ§ilen Model: Multi-Layer Perceptron (MLP)

MLP modeli, tam baÄŸlantÄ±lÄ± (fully connected) katmanlardan oluÅŸur ve genellikle yapÄ±sal verilerle Ã§alÄ±ÅŸan sÄ±nÄ±flandÄ±rma/regresyon problemlerinde tercih edilir. Bu projede kullanÄ±lacak model aÅŸaÄŸÄ±daki ÅŸekilde yapÄ±landÄ±rÄ±lmÄ±ÅŸtÄ±r:

### ğŸ”§ Model Mimarisi

| Katman              | Boyut | Aktivasyon Fonksiyonu |
|---------------------|-------|------------------------|
| Girdi KatmanÄ±       | 128   | -                      |
| Gizli Katman 1      | 64    | ReLU                   |
| Gizli Katman 2      | 32    | ReLU                   |
| Ã‡Ä±kÄ±ÅŸ KatmanÄ±       | 1     | Sigmoid                |

### ğŸ¯ Aktivasyon FonksiyonlarÄ±

- **ReLU (Rectified Linear Unit):** Gizli katmanlarda kullanÄ±lan yaygÄ±n bir aktivasyon fonksiyonudur. Negatif deÄŸerleri sÄ±fÄ±rlar, pozitif deÄŸerleri aynen geÃ§irir.
- **Sigmoid:** Ã‡Ä±kÄ±ÅŸ katmanÄ±nda tercih edilmiÅŸtir Ã§Ã¼nkÃ¼ projemiz **binary classification (ikili sÄ±nÄ±flandÄ±rma)** problemi Ã§Ã¶zmektedir.

## ğŸ“Œ Notlar

- Model `Keras` kullanÄ±larak `Sequential` API ile yapÄ±landÄ±rÄ±lacaktÄ±r.
- Daha ileri seviye iyileÅŸtirmeler iÃ§in dropout, batch normalization gibi teknikler ileriki aÅŸamalarda deÄŸerlendirilecektir.

---

âœ”ï¸ Bu issue kapsamÄ±nda modelin temel mimarisi ve aktivasyon fonksiyonlarÄ± baÅŸarÄ±yla belirlenmiÅŸ, uygulanabilir hale getirilmiÅŸtir.
